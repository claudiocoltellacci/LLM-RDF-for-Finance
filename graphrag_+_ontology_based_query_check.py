# -*- coding: utf-8 -*-
"""GraphRAG + Ontology-Based Query Check.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dy2Y4kCu2x7VlIVXYRlv6o6SiQSkFb2E

# GraphRAG+Ontology-Based Query Check for question answering

This project's final goal is to obtain an assistant for Financial Analysis combining LLMs and RDF Structured Data

## Library import and LLM loading
"""

! pip install langchain
! pip install langchain_community
! pip install langchain_mistralai
! pip install sentence_transformers
! pip install transformers
! pip install huggingface_hub
! pip install langchain_community
! pip install langchain_mistralai
! pip install langchain_experimental
! pip install mistralai
! pip install getpass
! pip install pypdf
! pip install faiss-gpu
! pip install faiss
! pip install faiss-cpu
! pip install rdflib
! pip install BeautifulSoup
! pip install SPARQLWrapper

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from pathlib import Path
from google.colab import runtime
import pandas as pd
import numpy as np
import torch
from transformers import BitsAndBytesConfig
from huggingface_hub import notebook_login
import re
from sentence_transformers import SentenceTransformer, util
from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD
import langchain
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_mistralai.embeddings import MistralAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from rdflib import Graph, RDF, RDFS, Namespace
from rdflib.plugins.sparql import prepareQuery
import requests
from bs4 import BeautifulSoup
import re
import pypdf
import faiss
from mistralai import Mistral
import requests
from getpass import getpass
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from rdflib import Graph, RDF, RDFS, Namespace, Literal
from rdflib.plugins.sparql import prepareQuery
from sentence_transformers import SentenceTransformer
from bs4 import BeautifulSoup
import requests
import re
import os
from typing import Dict, List, Optional
from SPARQLWrapper import SPARQLWrapper, JSON
from rdflib import Graph, URIRef, RDF, RDFS, Literal
from collections import defaultdict
from typing import Dict, List, Tuple

from google.colab import userdata
userdata.get('Thesis')
BEE_KEY = userdata.get('Thesis')
client = Mistral(BEE_KEY)

# If there's a GPU available...
if torch.cuda.is_available():
    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using the MPS device for MAC")
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

#Model Verification
def test_mistral_models(api_key):
    models = ["mistral-small-latest", "mistral-medium-latest","open-mistral-nemo","open-codestral-mamba","mistral-large-latest"]  # You can extend this if needed
    url = "https://api.mistral.ai/v1/chat/completions"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    for model in models:
        print(f"Testing model: {model}")
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": "Say hello!"}]
        }

        try:
            response = requests.post(url, headers=headers, json=payload)
            if response.status_code == 200:
                print(f"‚úÖ {model} responded successfully.")
                print("Response:", response.json()["choices"][0]["message"]["content"])
            else:
                print(f"‚ùå {model} failed. Status Code: {response.status_code}")
                print("Error:", response.json())
        except Exception as e:
            print(f"‚ö†Ô∏è Error testing {model}: {e}")

# Example usage:
test_mistral_models(BEE_KEY)

"""## Ontology Loading and Final Cleaning"""

from google.colab import drive
drive.mount('/content/drive')
Ontology = '/content/drive/MyDrive/ultimate_financial_ontology.ttl'

ontology = Graph()
ontology.parse(Ontology)
print(len(ontology.serialize(format="turtle").split()))

Ontology_cleaned = Graph()

for sub, pred, obj in ontology:
  if isinstance (obj, Literal) and obj.datatype == XSD.double:
    try:
      float(obj)
      Ontology_cleaned.add((sub, pred, obj))
    except ValueError:
      fixed_lexical = str(obj).replace(',', '.')
      try:
        new_obj = Literal(float(fixed_lexical), datatype=XSD.double)
        Ontology_cleaned.add((sub, pred, new_obj))
      except ValueError:
        print (f"Still broken: {obj}")
  else:
    Ontology_cleaned.add((sub, pred, obj))

Ontology_cleaned.serialize(destination="Polished_Ontology.ttl", format="turtle")
ontology.parse("Polished_Ontology.ttl")
print(len(Ontology_cleaned.serialize(format="turtle").split()))
full_graph = Ontology_cleaned

"""## Ontology chunking

Mistral's context window is 128K tokens, which is big but might be not enough for the ontology, also because of the noise produced by the syntaxis, despite being written in Turtle. This is why the Ontology is first cut into several parts based on the Industries.
"""

# Usa il grafo RDF gi√† pulito
full_graph = Ontology_cleaned

# Cartella di output
output_dir = "industry_modules"
Path(output_dir).mkdir(exist_ok=True)

# Namespace
ns = Namespace("http://art.uniroma2.it/ontologies/financial-ontology#")

# Lista delle industry
industry_list = [
    "Communication_Equipment", "Computer_Hardware", "Consumer_Electronics",
    "Electronic_Components", "Electronics_&_Computer_Distribution", "Information_Technology_Services",
    "Scientific_&_Technical_Instruments", "Semiconductor_Equipment_&_Materials", "Semiconductors",
    "Software_-_Application", "Software_-_Infrastructure", "Solar"
]

# Propriet√† rilevanti
IS_IN_INDUSTRY = ns.IsInIndustry
IS_IN_COUNTRY = ns.IsInCountry
IS_IN_REGION = ns.IsInRegion
FINANCIAL_DOC_PROPS = [
    ns.IncomeStatement,
    ns.BalanceSheet,
    ns.CashFlow,
    ns.SharesOutstanding
]

def add_full_description(g: Graph, target_graph: Graph, subject: URIRef):
    """Aggiunge tutte le triple del soggetto, e le label/comment di predicati e oggetti"""
    for p, o in g.predicate_objects(subject=subject):
        target_graph.add((subject, p, o))

        if isinstance(p, URIRef):
            for lp, lv in g.predicate_objects(subject=p):
                if lp in [RDFS.label, RDFS.comment]:
                    target_graph.add((p, lp, lv))

        if isinstance(o, URIRef):
            for pp, oo in g.predicate_objects(subject=o):
                target_graph.add((o, pp, oo))
                if isinstance(pp, URIRef):
                    for lpp, lvv in g.predicate_objects(subject=pp):
                        if lpp in [RDFS.label, RDFS.comment]:
                            target_graph.add((pp, lpp, lvv))

def extract_industry_module(industry_name: str):
    module_graph = Graph()
    industry_uri = ns[industry_name]

    # üîÅ Aggiungi TBox completo (classi + propriet√†)
    for s, p, o in full_graph.triples((None, RDF.type, OWL.Class)):
        module_graph.add((s, p, o))
    for s, p, o in full_graph.triples((None, RDF.type, OWL.ObjectProperty)):
        module_graph.add((s, p, o))
    for s, p, o in full_graph.triples((None, RDF.type, OWL.DatatypeProperty)):
        module_graph.add((s, p, o))
    for s, p, o in full_graph.triples((None, RDFS.subClassOf, None)):
        module_graph.add((s, p, o))
    for s, p, o in full_graph.triples((None, RDFS.domain, None)):
        module_graph.add((s, p, o))
    for s, p, o in full_graph.triples((None, RDFS.range, None)):
        module_graph.add((s, p, o))

    # üîÅ Aggiungi etichette e commenti su classi/propriet√†
    for t in full_graph.triples((None, RDFS.label, None)):
        module_graph.add(t)
    for t in full_graph.triples((None, RDFS.comment, None)):
        module_graph.add(t)

    # üîÅ Aggiungi le istanze legate all'industry selezionata
    for company in full_graph.subjects(predicate=IS_IN_INDUSTRY, object=industry_uri):
        add_full_description(full_graph, module_graph, company)

        for prop in FINANCIAL_DOC_PROPS:
            for doc in full_graph.objects(company, prop):
                add_full_description(full_graph, module_graph, doc)

        for loc_prop in [IS_IN_COUNTRY, IS_IN_REGION]:
            for loc in full_graph.objects(company, loc_prop):
                add_full_description(full_graph, module_graph, loc)

    output_path = os.path.join(output_dir, f"{industry_name}.ttl")
    module_graph.serialize(destination=output_path, format="turtle")
    print(f"‚úÖ Salvato modulo: {output_path}")

# Avvia generazione dei moduli
for industry in industry_list:
    extract_industry_module(industry)

# Esegui estrazione + stampa statistiche per tutte le Industry
for industry in industry_list:
    extract_industry_module(industry)

    # Calcolo statistiche
    file_path = os.path.join(output_dir, f"{industry}.ttl")
    g = Graph()
    g.parse(file_path, format="turtle")

    num_triples = len(g)
    num_companies = len(set(g.subjects(RDF.type, ns.Company)))

    print(f"üìä {industry} ‚Üí {num_companies} company ‚Äì {num_triples} triple")

from google.colab import files
import shutil

shutil.make_archive("industry_modules", 'zip', "industry_modules")
files.download("industry_modules.zip")

"""#Ontology Textualization

LLMs are trained on Texts. Therefore, to get better results, it might be useful to convert the sub-Knowledge Graphs into texts
"""

# Impostazioni
module_dir = "industry_modules"
output_dir = "industry_texts"
Path(output_dir).mkdir(exist_ok=True)

# Setup LLM
llm = ChatMistralAI(model="open-mistral-nemo", api_key=BEE_KEY)

# Prompt template
prompt_template = PromptTemplate(
    input_variables=["ontology"],
    template=(
"""You are a financial analyst and ontology expert. Given the following ontology module in Turtle format, generate a **detailed and comprehensive textual description** of each company. Your goal is to **translate every RDF triple**
    related to each company into **clear, natural language**.

For **each company**, follow this structure:

1. **General Overview**: Include revenues and costs variation, market capitalisation, future investments, amortisations, and dividends distributed to shareholders.
2. **Context**:
   - Describe the **industry** they belong to and the **country** they operate in.
   - Include any available indexes, labels, or other metadata.
3. **Financial Statements**:
   - **Income Statement**: Describe all available line items (e.g., EBIT, EBITDA, Normalized EBITDA, Tax Rate, Total Unusual Items, Net Income, Normalized Income, Interest Expense, Reconciled Cost of Revenue, Reconciled Depreciation,
       Tax Effect of Unusual Items, Total Unusual Items, Total Unusual Items Excluding Goodwill).
   - **Balance Sheet**: Describe all available line items (e.g. Cash and Cash Equivalents, Other Short Term Investments, Accounts Receivable, Allowance for Doubtful Accounts, Net Debt, Total Debt, Share Issued, Ordinary Shares Number,
       Treasury Shares Number, Tangible Book Value).
   - **Cash Flow Statement**: Describe all available line items (e.g. Beginning Cash Position, End Cash Position, Free Cash Floe, Supplemental Data, Issuance of Debt, Repayment of Debt, Issuance of Capital Stock, Capital Expenditure,
        Changes in Cash, Income Tax Paid Supplemental Data, Interest Paid Supplemental Data).
4. **Shares Outstanding**: Describe the number and variation if available.

**IMPORTANT**:
- **Rephrase** every RDF statement involving the company, its documents, and their properties in human-readable language.
- Do **not** skip any RDF predicate or literal.
- Do **not** use RDF or ontology-specific terms (e.g., "object property", "owl:Class"). Focus on financial storytelling.

Ontology Module:
{ontology}"""
    )
)

chain = LLMChain(llm=llm, prompt=prompt_template)

# Loop sui moduli
#for ttl_file in Path(module_dir).glob("*.ttl"):
 #   print(f"üîç Verbalizing: {ttl_file.name}")
  #  g = Graph()
   # g.parse(str(ttl_file), format="turtle")

    #turtle_content = g.serialize(format="turtle")
    #try:
     #   explanation = chain.run(ontology=turtle_content).strip()
    #except Exception as e:
     #   explanation = f"‚ö†Ô∏è Error generating text: {e}"

    #out_path = Path(output_dir) / (ttl_file.stem + ".txt")
    #with open(out_path, "w", encoding="utf-8") as f:
     #   f.write(explanation)

    #print(f"‚úÖ Saved: {out_path}")

sa= "/content/industry_modules/Software_-_Application.ttl"
g = Graph()
g.parse(sa, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(sa).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

si= "/content/industry_modules/Software_-_Infrastructure.ttl"
g = Graph()
g.parse(si, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(si).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

so= "/content/industry_modules/Solar.ttl"
g = Graph()
g.parse(so, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(so).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

sc= "/content/industry_modules/Semiconductors.ttl"
g = Graph()
g.parse(sc, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(sc).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

se= "/content/industry_modules/Semiconductor_Equipment_&_Materials.ttl"
g = Graph()
g.parse(se, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(se).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

st= "/content/industry_modules/Scientific_&_Technical_Instruments.ttl"
g = Graph()
g.parse(st, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(st).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

it= "/content/industry_modules/Information_Technology_Services.ttl"
g = Graph()
g.parse(it, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(it).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

et= "/content/industry_modules/Electronics_&_Computer_Distribution.ttl"
g = Graph()
g.parse(et, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(et).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

ec= "/content/industry_modules/Electronic_Components.ttl"
g = Graph()
g.parse(ec, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(ec).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

ce= "/content/industry_modules/Consumer_Electronics.ttl"
g = Graph()
g.parse(ce, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(ce).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

ch= "/content/industry_modules/Computer_Hardware.ttl"
g = Graph()
g.parse(ch, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(ch).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

com= "/content/industry_modules/Communication_Equipment.ttl"
g = Graph()
g.parse(com, format="turtle")
turtle_content = g.serialize(format="turtle")
explanation = chain.run(ontology=turtle_content).strip()
print(explanation)
out_path = Path(output_dir) / (Path(com).stem + ".txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(explanation)
print(f"‚úÖ Saved: {out_path}")

"""#Prompt Similarity

The assistant should be able to recognise which sub-Knowledge Graph is fit for the information retrieval task. We will use spaCy's functions
"""

question = "What's the financial situation of Oracle?"

"""Light model but could be ineffective"""

!python -m spacy download en_core_web_sm

import spacy

nlp = spacy.load("en_core_web_sm")  # Modello leggero ma efficace, devi scrivere il nome esatto della societ√†

def extract_company_name(question: str) -> str:
    doc = nlp(question)
    for ent in doc.ents:
        if ent.label_ == "ORG":
            return ent.text
    return question.strip()

"""Heavy model but most certainly effective"""

!python -m spacy download en_core_web_trf

import spacy

nlp = spacy.load("en_core_web_trf")  # Modello pesante ma estremamente efficace

def extract_company_name(question: str) -> str:
    doc = nlp(question)
    for ent in doc.ents:
        if ent.label_ == "ORG":
            return ent.text
    return question.strip()

print(extract_company_name(question))
company_name = extract_company_name(question)
print(company_name)

for s,p,o in full_graph.triples((None, RDFS.label, None)):
  print(s,p,o)

def extract_company_labels_custom_label(graph: Graph) -> dict:
    """
    Extracts rdf:label values from instances of ns1:Company (financial-ontology#Company),
    where labels are stored under rdf:label instead of rdfs:label.
    """


    ns1 = Namespace("http://art.uniroma2.it/ontologies/financial-ontology#")
    rdf_label = URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#label")  # your dataset's label usage

    companies = {}

    for s in graph.subjects(RDF.type, ns1.Company):
        labels = [str(o) for o in graph.objects(s, rdf_label) if isinstance(o, Literal)]
        if labels:
            companies[s] = labels

    return companies

labels_dict = extract_company_labels_custom_label(full_graph)

print(labels_dict)

companies = extract_company_labels_custom_label(full_graph)
for uri, labels in companies.items():
    print(f"üè¢ {uri}")
    for label in labels:
        print(f"   üîπ {label}")

def find_company_module(question: str, ontology_dir: str = "industry_modules") -> str:
    ns1 = Namespace("http://art.uniroma2.it/ontologies/financial-ontology#")
    rdf_label = URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#label")
    company_name = extract_company_name(question)

    for ttl_file in Path(ontology_dir).glob("*.ttl"):
        graph = Graph()
        graph.parse(str(ttl_file), format="turtle")

        for s in graph.subjects(RDF.type, ns1.Company):
          labels = [str(o) for o in graph.objects(s, rdf_label) if isinstance(o, Literal)]
          if any(company_name.lower() in label.lower() for label in labels):
            return ttl_file.name

    return None

module_found = find_company_module(question)
if module_found:
    print(f"‚úÖ Company trovata nel modulo: {module_found}")
else:
    print("‚ùå Nessuna corrispondenza trovata")

from pathlib import Path

def get_matching_paths(module_name: str,
                       ttl_dir="industry_modules",
                       txt_dir="industry_texts") -> tuple:
    ttl_path = Path(ttl_dir) / f"{module_name}.ttl"
    txt_path = Path(txt_dir) / f"{module_name}.txt"
    return str(ttl_path), str(txt_path)

ttl_file, txt_file = get_matching_paths(Path(module_found).stem)

print(f"TTL file: {ttl_file}")
print(f"TXT file: {txt_file}")

# Carica il grafo per SPARQL
g = Graph().parse(ttl_file, format="turtle")

print(g.serialize(format="turtle"))

"""#Ontology-Based Query Check

Now the Ontology is used for writing a SPARQL query which will be written, checked and edited by the LLM
"""

def generate_sparql_with_mistral(companyname: str, ontology_ttl: str) -> str:
    llm = ChatMistralAI(model="codestral-latest", api_key=BEE_KEY)
    prompt_template = PromptTemplate(
        input_variables=["ontology", "company_name"],
        template=("""Given the OWL model described in the following TTL file:
                      {ontology}

                      And the name of a company:
                      {company_name}

                      Replace 'placeholder' in the SPARQL query below to filter results for the given company name. Ensure that the syntax is valid and the query is executable.

                      Return ONLY the updated SPARQL query‚Äîdo not explain anything.

                      PREFIX financial: <http://art.uniroma2.it/ontologies/financial-ontology#>
                      PREFIX dc: <http://purl.org/dc/elements/1.1/>
                      PREFIX grddl: <http://www.w3.org/2003/g/data-view#>
                      PREFIX owl: <http://www.w3.org/2002/07/owl#>
                      PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
                      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                      PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

                      SELECT * WHERE {{
                        VALUES ?statementType {{
                          financial:MarketCapitalization
                          financial:Dividends
                          financial:RevenueVariation
                          financial:CostVariation
                          financial:IsInCountry
                          financial:IsInIndustry
                          financial:IsInSector
                          financial:IncomeStatement
                          financial:BalanceSheet
                          financial:CashFlow
                          financial:SharesOutstanding
                        }}
                        ?company rdf:label ?label.
                        FILTER(regex(str(?label), "placeholder", "i"))
                        ?company ?statementType ?statement .
                        ?statement ?property ?value .
                        FILTER (isLiteral(?value))
                      }}
                      ORDER BY ?statementType ?statement ?property
                      """
            )
    )

    chain = LLMChain(llm=llm, prompt=prompt_template)
    result = chain.run({"ontology": ontology_ttl, "company_name": company_name})
    print(result)  # This will print the generated SPARQL query
    return result.strip()  # Return the query, stripping any extra whitespace

def check_domain_violation(query_graph: Graph, ontology_graph: Graph) -> list:
    violations = []
    for s, p, o in query_graph.triples((None, None, None)):
        domain = ontology_graph.value(subject=p, predicate=RDFS.domain)
        if domain and not (s, RDF.type, domain) in query_graph:
            violations.append({
                "type": "domain",
                "property": p,
                "expected_domain": domain,
                "subject": s
            })
    return violations

def check_range_violation(query_graph: Graph, ontology_graph: Graph) -> list:
    violations = []
    for s, p, o in query_graph.triples((None, None, None)):
        range_ = ontology_graph.value(subject=p, predicate=RDFS.range)
        if range_ and not (o, RDF.type, range_) in query_graph:
            violations.append({
                "type": "range",
                "property": p,
                "expected_range": range_,
                "object": o
            })
    return violations

def explain_violation(violation: dict) -> str:
    if violation["type"] == "domain":
        return (f"The property {violation['property']} has domain {violation['expected_domain']}, "
                f"but its subject {violation['subject']} does not match or inherit from it.")
    elif violation["type"] == "range":
        return (f"The property {violation['property']} has range {violation['expected_range']}, "
                f"but its object {violation['object']} does not match or inherit from it.")
    return "Unknown violation."

def repair_query_with_mistral(bad_query: str, ontology_str: str, violations: list, api_key: str) -> str:
    llm = ChatMistralAI(model="codestral-latest", api_key=api_key)
    prompt = PromptTemplate(
        input_variables=["ontology", "query", "issues"],
        template=("You are an expert in RDF and SPARQL. Given the following ontology in Turtle format:\n\n"
                  "{ontology}\n\n"
                  "The following SPARQL query has the following issues:\n\n"
                  "{issues}\n\n"
                  "Please rewrite the query to fix the identified issues. ONLY return the corrected SPARQL query.\n"
                  "If no issues are found or the query is already correct, return the original query unchanged.\n\n"
                  "Query:\n{query}")
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    response = chain.run({
        "ontology": ontology_str,
        "query": bad_query,
        "issues": "\n".join(violations)
    })
    return response.strip()

import re

def clean_sparql_output(raw_query: str) -> str:
    """
    Cleans LLM-generated SPARQL query by removing markdown backticks.
    """
    return re.sub(r"^```sparql\s*|```$", "", raw_query.strip(), flags=re.MULTILINE).strip()

sparql_query = generate_sparql_with_mistral(company_name, g.serialize(format="turtle"))
print(sparql_query)
sparql_query = clean_sparql_output(sparql_query)
print(sparql_query)

financial = Namespace("http://art.uniroma2.it/ontologies/financial-ontology#")
print(financial)

import re
from rdflib import Graph, Namespace, BNode
def extract_triples_from_query(query_str: str, ns: Namespace, expected_prefix: str = "financial") -> Graph:
      extract = Graph()
      bnodes = []

    # Find VALUES clause with predicates
      pattern = re.compile(r"VALUES\s+\?\w+\s*{([^}]*)}", re.MULTILINE)
      values_match = pattern.search(query_str)

      if not values_match:
        print("No VALUES block with prefixed predicates found.")
        return extract

      values_block = values_match.group(1)

      # Match entries like financial:Dividends
      predicate_matches = re.findall(r"([a-zA-Z0-9_]+):(\w+)", values_block)

      for prefix, pred in predicate_matches:
        if prefix != expected_prefix:
            print(f"Skipping prefix: {prefix}")
            continue

        # Create synthetic triple: _:s financial:pred _:o
        s = BNode()
        o = BNode()
        p = ns[pred]
        extract.add((s, p, o))

      print(f"Extracted {len(extract)} triples from VALUES block.")
      return extract
print(extract_triples_from_query(sparql_query, financial))

query_graph = extract_triples_from_query(sparql_query, financial)
print(query_graph)

domain_violations = check_domain_violation(query_graph, g)
range_violations = check_range_violation(query_graph, g)

print(domain_violations)
print(range_violations)

complete_violations = domain_violations + range_violations
print(complete_violations)

explanation = ""  # Initialize explanation as an empty string
for violation in complete_violations:
    explanation += explain_violation(violation) + "\n"  # Concatenate individual explanations

print(explanation)

if explanation == "":
  repaired_query = sparql_query
else:
    repaired_query = repair_query_with_mistral(sparql_query, g.serialize(format="turtle"), explanation, BEE_KEY)
    print(repaired_query)

def clean_sparql_output(raw_query: str) -> str:
    """
    Cleans LLM-generated SPARQL query by removing markdown backticks.
    """
    return re.sub(r"^```sparql\s*|```$|The provided SPARQL query has several issues related to incorrect or mismatched prefixes and properties. Below is the corrected SPARQL query\\:|The main corrections include ensuring that the prefixes are correctly defined and used throughout the query\\. The original query had issues with incorrect or mismatched prefixes\\, which have been fixed\\.", "", raw_query.strip(), flags=re.MULTILINE).strip()

def extract_sparql_query_only(text: str) -> str:
    """
    Extracts only the SPARQL query from an LLM-generated response.
    Starts from first PREFIX line and stops when all opened braces are closed.
    """
    lines = text.strip().splitlines()
    query_lines = []
    brace_count = 0
    in_query = False

    for line in lines:
        stripped = line.strip()

        # Start collecting at first PREFIX
        if stripped.startswith("PREFIX"):
            in_query = True

        if in_query:
            query_lines.append(line)

            # Track braces to know when WHERE clause is done
            brace_count += stripped.count("{")
            brace_count -= stripped.count("}")

            # Exit when all opened braces are closed and query has started
            if brace_count <= 0 and stripped.startswith("}"):
                break

    return "\n".join(query_lines).strip()

repaired_query = extract_sparql_query_only(repaired_query)

print(repaired_query)

results = g.query(repaired_query)
for row in results:
    print(row)

def format_results_for_llm(results) -> str:
    formatted = []
    for row in results:
        row_str = []
        for i, col in enumerate(row):
            # Convert each RDF term into a string
            if hasattr(col, "n3"):
                row_str.append(f"{col.n3()}")
            else:
                row_str.append(str(col))
        formatted.append(" | ".join(row_str))
    return "\n".join(formatted)
formatted_results = format_results_for_llm(results)
print(formatted_results)

"""# RAG

Now we apply RAG to the model, so that it can provide an answer based on the context provided with the Knowledge Graph and the RDF triples retrievd via SPARQL
"""

llm = ChatMistralAI(model="mistral-small-latest", api_key=BEE_KEY)

# Prompt template--NEEDS WORK
prompt_template = PromptTemplate(
    input_variables=["results"],
    template=(
        "You are an expert in finance and ontologies. "
        "Given the following SPARQL query result, describe the company's"
        "financial situation, making explicit reference to its Financial Statement\n"
        "Return: \n"
        "- its general situation taking into account its variation in costs and revenues,"
        "  its market capitalisation, its future investments and amortisations, and the amount of dividends it distributes to shareholders\n"
        "- describe the industry and country they are operating in, listing their properties and indexes\n"
        "- its IncomeStatement\n"
        "- its BalanceSheet\n"
        "- its CashFlow\n"
        "- its SharesOutstanding\n"

        "Avoid technical RDF language. Be as specific as possible. For each property or item in the financial statements, report its **value**.\n\n"
        "sparql_graph:\n{results}"
    )
)
chain = LLMChain(llm=llm, prompt=prompt_template)
description_sparql = chain.run(results=formatted_results).strip()

print(description_sparql)

from sentence_transformers import SentenceTransformer, util
from langchain.embeddings import SentenceTransformerEmbeddings
#Chunking the subGraph Text
loader = TextLoader(txt_file)
documents = loader.load()
text_splitter = SemanticChunker(SentenceTransformerEmbeddings(), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use
texts = [doc.page_content for doc in documents]
documentation = text_splitter.create_documents(texts)
#Chunking the query result
description_chunks = text_splitter.create_documents([description_sparql])
# Define the embedding model
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = SentenceTransformerEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
# Create the vector store
all_documents = documentation + description_chunks
vector = FAISS.from_documents(all_documents, embeddings)
# Define a retriever interface
retriever = vector.as_retriever()
# Define LLM
model = ChatMistralAI(model = "mistral-large-latest",mistral_api_key=BEE_KEY)
prompt = ChatPromptTemplate.from_template("""You are Hiroshi Abe, a helpful and expert AI Financial Assistant specializing in fundamental analysis of companies.

Your role is to analyze a company‚Äôs financial health based on the provided data sources. You MUST only use the two sources below to generate your answer.

When analyzing a company's financial situation, follow these steps:

1. **PEST Analysis**
   Perform a Political, Economic, Social, and Technological (PEST) analysis relevant to the firm.

2. **Financial Ratios Evaluation**
   Calculate and interpret:
   - **Return on Equity (ROE)**
   - **Return on Investment (ROI)**
   - **Return on Assets (ROA)**
   Use the following formulas for assessment:
   ‚Ä¢ ROE ‚â• *r_f* + *Œ≤_i* √ó (*r_M* ‚àí *r_f*)
   ‚Ä¢ ROI ‚â• ROE
   Where:
   - *r_f* = 4% (US Treasury Bond Return Rate)
   - *r_M* = 23.63% (Technology Sector Market Return Rate)
   - *Œ≤_i* = Industry Beta
   Explain each computation you do, highlighting every line item used
   Conclude whether the financial situation is **Good** or **Not Good** based on these thresholds.

3. **Forecasting**
   Provide 5-year forecasted:
   - **Balance Sheets**
   - **Income Statements**
   These forecasts must be based on estimated changes in **costs**, **revenues**, and **amortization schedules**.

4. **Firm Valuation (DCF Method)**
   Compute the firm‚Äôs valuation using the **Discounted Cash Flow (DCF)** method.

---

 **Answer the question based only on the provided context:**
  {context}

---

 **User Question:**
{input}"""
)

# Create a retrieval chain to answer questions
document_chain = create_stuff_documents_chain(model, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)
response = retrieval_chain.invoke({"input": question})
print(response["answer"])

"""As "GSM-Symbolic" by Mirzadeh reports, LLMs do not understand how math works. Therefore, it was decided to use another prompt for context analysis and Balance Sheet Retrieval."""

from sentence_transformers import SentenceTransformer, util
from langchain.embeddings import SentenceTransformerEmbeddings
#Chunking the subGraph Text
loader = TextLoader(txt_file)
documents = loader.load()
text_splitter = SemanticChunker(SentenceTransformerEmbeddings(), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use
texts = [doc.page_content for doc in documents]
documentation = text_splitter.create_documents(texts)
#Chunking the query result
description_chunks = text_splitter.create_documents([description_sparql])
# Define the embedding model
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = SentenceTransformerEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
# Create the vector store
all_documents = documentation + description_chunks
vector = FAISS.from_documents(all_documents, embeddings)
# Define a retriever interface
retriever = vector.as_retriever()
# Define LLM
model = ChatMistralAI(model = "mistral-large-latest",mistral_api_key=BEE_KEY)
prompt = ChatPromptTemplate.from_template("""You are Hiroshi Abe, a helpful and expert AI Financial Assistant specializing in fundamental analysis of companies.

Your role is to provide context information and help analysts make fundamental analysis. You MUST only use the two sources below to generate your answer.

When retrieving data about a company, follow these steps:

1. **PEST Analysis**
   Analyse the company from a macro perspective, according to the PEST Analysis paradigm:
   - **Political**
   - **Economic**
   - **Social**
   - **Technological**
  For each point, write a brief comment on the company's situation.

2. **General Data**
Return general information, including:
- **Market Capitalization**
- **Industry**
- **Country**
- **Variation in Costs and Revenues**
- **Future Investments**
- **Shares Outstanding**.

3. **Financial Statements**
Return its Financial Statement:
   - **Income Statement**
   - **Balance Sheet**
   - **Cash Flow**

Use bullet points or structured paragraphs for each section. Keep the language clear and professional. Be concise but thorough.
---

 **Answer the question based only on the provided context:**
  {context}

---

 **User Question:**
{input}"""
)

# Create a retrieval chain to answer questions
document_chain = create_stuff_documents_chain(model, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)
response = retrieval_chain.invoke({"input": question})
print(response["answer"])